{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turned-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "prolific_df = pd.read_excel(\"Prolific_Data_All_with_time.xlsx\", sheet_name=\"Sheet1\")\n",
    "negative_low_df = pd.read_excel(\"Negative_Low_with_time.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Append / concatenate the two datasets\n",
    "df = pd.concat([prolific_df, negative_low_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worst-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if subject didnt complete the experiment -->'Earning' is missing\n",
    "df = df[df['Earning'].notna()]\n",
    "\n",
    "# Drop subjects who returned their submission based on Prolific_ID\n",
    "returners = [\n",
    "    \"61549a2caa92d84f392000e9\", \"5ec62902bb44c16b842128f9\", \"56b0ac5fe8b677000cdc0a34\",\n",
    "    \"65dd365163f7579fba938dea\", \"5c2fcd716ea6880001dc8e3d\", \"63e51c6b462cfa8a42eba8b2\",\n",
    "    \"6571af52592a29d0854f366e\", \"6234d77291179c1badffad36\", \"63c09186d75a9bbd3bec5652\",\n",
    "    \"5ffa5b733bd20c42d0b40b87\", \"5ced23c585712e00190bd98a\", \"66154557c058526ce9f1cfe2\"\n",
    "]\n",
    "\n",
    "df = df[~df['Prolific_ID'].isin(returners)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instructional-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-investors and those not fluent in English\n",
    "df = df[(df['Fin_market'] != 2) & (df['English'] != 2)]\n",
    "\n",
    "# Drop participants who made 3 mistakes and failed the attention check\n",
    "df = df[~((df['correct_count'] == 3) & (df['P2_AttenCheck_2'] != 5))]\n",
    "\n",
    "# Create a participant ID \n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Map treatment conditions to numeric codes\n",
    "treatment_map = {\n",
    "    'Baseline': 1,\n",
    "    'Positive_Higher': 2,\n",
    "    'Negative': 3,\n",
    "    'Positive_Lower': 4\n",
    "}\n",
    "df['treatment_num'] = df['treatment'].map(treatment_map)\n",
    "\n",
    "# Drop \"Positive_Lower\" condition (treatment_num == 4) as we do not use it in the main analyses\n",
    "df = df[df['treatment_num'] != 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joint-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Gender: 1 -> 0, 2 -> 1, 3/4 -> 0\n",
    "df['Gender'] = df['Gender'].replace({1: 0, 2: 1, 3: 0, 4: 0})\n",
    "\n",
    "# Recode Intere_inves and Own_asset: 2 -> 0\n",
    "df['Intere_inves'] = df['Intere_inves'].replace({2: 0})\n",
    "df['Own_asset'] = df['Own_asset'].replace({2: 0})\n",
    "\n",
    "# Trade_frequently: 1 if Trade_freq < 4\n",
    "df['Trade_frequently'] = (df['Trade_freq'] < 4).astype(int)\n",
    "\n",
    "# High_wealth: 1 if Inve_amount >= 4 and not 7 (7 is \"don't know\")\n",
    "df['High_wealth'] = ((df['Inve_amount'] >= 4) & (df['Inve_amount'] != 7)).astype(int)\n",
    "\n",
    "# High_CurrentSustainShare: 1 if Sustain_share >= 4 and not 7\n",
    "df['High_CurrentSustainShare'] = ((df['Sustain_share'] >= 4) & (df['Sustain_share'] != 7)).astype(int)\n",
    "\n",
    "# Invest_in_SRI: 1 if Sustain_share != 1 and != 7\n",
    "df['Invest_in_SRI'] = ((df['Sustain_share'] != 1) & (df['Sustain_share'] != 7)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confident-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Share_willing and Donate_charity\n",
    "df['Share_willing_std'] = (df['Share_willing'] - df['Share_willing'].mean()) / df['Share_willing'].std()\n",
    "df['Donate_charity_std'] = (df['Donate_charity'] - df['Donate_charity'].mean()) / df['Donate_charity'].std()\n",
    "\n",
    "# Compute the Falk et al. (2023)-based Social Preferences measure\n",
    "df['Social_Preferences'] = (\n",
    "    0.635 * df['Share_willing_std'] +\n",
    "    0.365 * df['Donate_charity_std']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjusted-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal share allocation benchmarks based on performed optimization calculations \n",
    "df['HighCorr_OptimalShare_lowreturn'] = 23.90\n",
    "df['LowCorr_OptimalShare_lowreturn'] = 45.60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "musical-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental Literacy Questions\n",
    "df['correct_Environ_Literacy1'] = (df['Environ_Literacy1'] == 2).astype(int)\n",
    "df['correct_Environ_Literacy2'] = (df['Environ_Literacy2'] == 2).astype(int)\n",
    "df['correct_Environ_Literacy3'] = (df['Environ_Literacy3'] == 1).astype(int)\n",
    "df['correct_Environ_Literacy4'] = (df['Environ_Literacy4'] == 1).astype(int)\n",
    "df['correct_Environ_Literacy5'] = (df['Environ_Literacy5'] == 4).astype(int)\n",
    "\n",
    "# Total Environmental Literacy Score\n",
    "df['correct_Environ_Literacy'] = (\n",
    "    df['correct_Environ_Literacy1'] +\n",
    "    df['correct_Environ_Literacy2'] +\n",
    "    df['correct_Environ_Literacy3'] +\n",
    "    df['correct_Environ_Literacy4'] +\n",
    "    df['correct_Environ_Literacy5']\n",
    ")\n",
    "\n",
    "# Financial Literacy Questions\n",
    "df['correct_Fin_lite_Q1'] = (df['Fin_lite_Q1'] == 2).astype(int)\n",
    "df['correct_Fin_lite_Q2'] = (df['Fin_lite_Q2'] == 1).astype(int)\n",
    "df['correct_Fin_lite_Q3'] = (df['Fin_lite_Q3'] == 3).astype(int)\n",
    "df['correct_Fin_lite_Q4'] = (df['Fin_lite_Q4'] == 2).astype(int)\n",
    "\n",
    "# Financial literacy without Q1 (as in lusardi and mitchell (2014) financial literacy score)\n",
    "df['numcorrect_financialliteracy'] = (\n",
    "    df['correct_Fin_lite_Q2'] +\n",
    "    df['correct_Fin_lite_Q3'] +\n",
    "    df['correct_Fin_lite_Q4']\n",
    ")\n",
    "\n",
    "# Financial literacy including all questions\n",
    "df['numcorrect_financialliteracyall'] = (\n",
    "    df['correct_Fin_lite_Q1'] +\n",
    "    df['correct_Fin_lite_Q2'] +\n",
    "    df['correct_Fin_lite_Q3'] +\n",
    "    df['correct_Fin_lite_Q4']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "northern-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehension Questions: correct answers\n",
    "df['correct_PosiRela1'] = (df['PosiRela1'] == \"drop\").astype(int)\n",
    "df['correct_PosiRela2'] = (df['PosiRela2'] == \"rise\").astype(int)\n",
    "df['correct_NegaRela1'] = (df['NegaRela1'] == \"rise\").astype(int)\n",
    "df['correct_NegaRela2'] = (df['NegaRela2'] == \"drop\").astype(int)\n",
    "\n",
    "# Total comprehension score\n",
    "df['correct_comprehension'] = (\n",
    "    df['correct_PosiRela1'] +\n",
    "    df['correct_PosiRela2'] +\n",
    "    df['correct_NegaRela1'] +\n",
    "    df['correct_NegaRela2']\n",
    ")\n",
    "\n",
    "# Drop participants who got 0 comprehension questions correctly to ensure quality responses \n",
    "df = df[df['correct_comprehension'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bronze-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1 investment asset by return level\n",
    "df['P1_investmentAsset_highreturn'] = df['P1_investmentAsset'].where(df['P1_high_return'] == 1, df['P1_investmentAsset2'])\n",
    "df['P1_investmentAsset_lowreturn'] = df['P1_investmentAsset'].where(df['P1_high_return'] == 0, df['P1_investmentAsset2'])\n",
    "\n",
    "# P2 investment asset by return level\n",
    "df['P2_investmentAsset_highreturn'] = df['P2_investmentAsset'].where(df['P1_high_return'] == 1, df['P2_investmentAsset2'])\n",
    "df['P2_investmentAsset_lowreturn'] = df['P2_investmentAsset'].where(df['P1_high_return'] == 0, df['P2_investmentAsset2'])\n",
    "\n",
    "# Assets assigned to correlation order\n",
    "df['LowCorr_investmentAsset_highret'] = df['P1_investmentAsset_highreturn'].where(df['P1_low_first'] == 1, df['P2_investmentAsset_highreturn'])\n",
    "df['HighCorr_investmentAsset_highret'] = df['P1_investmentAsset_highreturn'].where(df['P1_low_first'] == 0, df['P2_investmentAsset_highreturn'])\n",
    "\n",
    "df['LowCorr_investmentAsset_lowret'] = df['P1_investmentAsset_lowreturn'].where(df['P1_low_first'] == 1, df['P2_investmentAsset_lowreturn'])\n",
    "df['HighCorr_investmentAsset_lowret'] = df['P1_investmentAsset_lowreturn'].where(df['P1_low_first'] == 0, df['P2_investmentAsset_lowreturn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generic-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert allocations to percentages\n",
    "df['LowCorr_investmentAsset_highret'] = (df['LowCorr_investmentAsset_highret'] / 10000) * 100\n",
    "df['LowCorr_investmentAsset_lowret'] = (df['LowCorr_investmentAsset_lowret'] / 10000) * 100\n",
    "\n",
    "df['HighCorr_investmentAsset_highret'] = (df['HighCorr_investmentAsset_highret'] / 10000) * 100\n",
    "df['HighCorr_investmentAsset_lowret'] = (df['HighCorr_investmentAsset_lowret'] / 10000) * 100\n",
    "\n",
    "# Define key variables for analysis\n",
    "df['correlation_neglect'] = df['HighCorr_investmentAsset_lowret'] - df['LowCorr_investmentAsset_lowret']\n",
    "df['consider_corr'] = df['LowCorr_investmentAsset_lowret'] - df['HighCorr_investmentAsset_lowret']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cubic-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert beliefs from dollar to percent for more intuitive interpretation\n",
    "belief_vars = [\n",
    "    'P1_belief_upper', 'P1_belief_lower', 'P1_Expect_value',\n",
    "    'P2_belief_upper', 'P2_belief_lower', 'P2_Expect_value'\n",
    "]\n",
    "\n",
    "for var in belief_vars:\n",
    "    df[var] = (df[var] - 10000) / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "satisfactory-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception of dependence between assets' returns\n",
    "# Relationship perception (probabilities)\n",
    "df['LowCorr_Relationship4'] = df['P1_Relationship4'].where(df['P1_low_first'] == 1, df['P2_Relationship4_2'])\n",
    "df['HighCorr_Relationship4'] = df['P1_Relationship4'].where(df['P1_low_first'] == 0, df['P2_Relationship4_2'])\n",
    "\n",
    "df['LowCorr_Relationship5'] = df['P1_Relationship5'].where(df['P1_low_first'] == 1, df['P2_Relationship5_2'])\n",
    "df['HighCorr_Relationship5'] = df['P1_Relationship5'].where(df['P1_low_first'] == 0, df['P2_Relationship5_2'])\n",
    "\n",
    "# Directional correctness\n",
    "df['P1_Relationship2_'] = (\n",
    "    ((df['P1_Relationship2'] == 'rise') & (df['P1_low_first'] == 1)) |\n",
    "    ((df['P1_Relationship2'] == 'drop') & (df['P1_low_first'] == 0))\n",
    ").astype(int)\n",
    "\n",
    "df['P1_Relationship3_'] = (\n",
    "    ((df['P1_Relationship3'] == 'drop') & (df['P1_low_first'] == 1)) |\n",
    "    ((df['P1_Relationship3'] == 'rise') & (df['P1_low_first'] == 0))\n",
    ").astype(int)\n",
    "\n",
    "df['P2_Relationship2'] = (\n",
    "    ((df['P2_Relationship2_2'] == 'drop') & (df['P1_low_first'] == 1)) |\n",
    "    ((df['P2_Relationship2_2'] == 'rise') & (df['P1_low_first'] == 0))\n",
    ").astype(int)\n",
    "\n",
    "df['P2_Relationship3'] = (\n",
    "    ((df['P2_Relationship3_2'] == 'drop') & (df['P1_low_first'] == 0)) |\n",
    "    ((df['P2_Relationship3_2'] == 'rise') & (df['P1_low_first'] == 1))\n",
    ").astype(int)\n",
    "\n",
    "# Total correct answers for perceived dependence\n",
    "df['correct_dependence'] = (\n",
    "    df['P1_Relationship2_'] +\n",
    "    df['P1_Relationship3_'] +\n",
    "    df['P2_Relationship2'] +\n",
    "    df['P2_Relationship3']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "rubber-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate each row for round-based reshaping\n",
    "df_round1 = df.copy()\n",
    "df_round1['First_Round'] = 1\n",
    "df_round1['Share_lowreturn'] = df_round1['P1_investmentAsset_lowreturn']\n",
    "df_round1['Expect_value'] = df_round1['P1_Expect_value']\n",
    "df_round1['Lower_Bound'] = df_round1['P1_belief_lower']\n",
    "df_round1['Upper_Bound'] = df_round1['P1_belief_upper']\n",
    "\n",
    "df_round2 = df.copy()\n",
    "df_round2['First_Round'] = 0\n",
    "df_round2['Share_lowreturn'] = df_round2['P2_investmentAsset_lowreturn']\n",
    "df_round2['Expect_value'] = df_round2['P2_Expect_value']\n",
    "df_round2['Lower_Bound'] = df_round2['P2_belief_lower']\n",
    "df_round2['Upper_Bound'] = df_round2['P2_belief_upper']\n",
    "\n",
    "# Combine into one dataframe\n",
    "df_long = pd.concat([df_round1, df_round2], ignore_index=True)\n",
    "\n",
    "# Define correlation type based on order and round\n",
    "df_long['High_Corr'] = ((df_long['First_Round'] == 1) & (df_long['P1_low_first'] == 0)) | \\\n",
    "                       ((df_long['First_Round'] == 0) & (df_long['P1_low_first'] == 1))\n",
    "df_long['High_Corr'] = df_long['High_Corr'].astype(int)\n",
    "\n",
    "df_long['Low_Corr'] = 1 - df_long['High_Corr']\n",
    "\n",
    "# Convert Share_lowreturn to percentage\n",
    "df_long['Share_lowreturn'] = (df_long['Share_lowreturn'] / 10000) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "organized-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign Belief Variables (Return Expectations & Bounds) to Asset Correlation\n",
    "# Expected return\n",
    "df['LowCorr_Expect_value'] = df['P1_Expect_value'].where(df['P1_low_first'] == 1, df['P2_Expect_value'])\n",
    "df['HighCorr_Expect_value'] = df['P1_Expect_value'].where(df['P1_low_first'] == 0, df['P2_Expect_value'])\n",
    "\n",
    "df['Avg_Expect_value'] = (df['LowCorr_Expect_value'] + df['HighCorr_Expect_value']) / 2\n",
    "df['Diff_Expect_value'] = df['LowCorr_Expect_value'] - df['HighCorr_Expect_value']\n",
    "\n",
    "# Lower bound\n",
    "df['LowCorr_belief_lower'] = df['P1_belief_lower'].where(df['P1_low_first'] == 1, df['P2_belief_lower'])\n",
    "df['HighCorr_belief_lower'] = df['P1_belief_lower'].where(df['P1_low_first'] == 0, df['P2_belief_lower'])\n",
    "\n",
    "df['Avg_belief_lower'] = (df['LowCorr_belief_lower'] + df['HighCorr_belief_lower']) / 2\n",
    "df['Diff_belief_lower'] = df['LowCorr_belief_lower'] - df['HighCorr_belief_lower']\n",
    "\n",
    "# Upper bound\n",
    "df['LowCorr_belief_upper'] = df['P1_belief_upper'].where(df['P1_low_first'] == 1, df['P2_belief_upper'])\n",
    "df['HighCorr_belief_upper'] = df['P1_belief_upper'].where(df['P1_low_first'] == 0, df['P2_belief_upper'])\n",
    "\n",
    "df['Avg_belief_upper'] = (df['LowCorr_belief_upper'] + df['HighCorr_belief_upper']) / 2\n",
    "df['Diff_belief_upper'] = df['LowCorr_belief_upper'] - df['HighCorr_belief_upper']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instant-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return perception: gain/loss probabilities (Q1â€“Q3)\n",
    "df['LowCorr_Return_perception1'] = df['P1_Return_perception1'].where(df['P1_low_first'] == 1, df['P2_Return_perception1'])\n",
    "df['HighCorr_Return_perception1'] = df['P1_Return_perception1'].where(df['P1_low_first'] == 0, df['P2_Return_perception1'])\n",
    "\n",
    "df['LowCorr_Return_perception2'] = df['P1_Return_perception2'].where(df['P1_low_first'] == 1, df['P2_Return_perception2'])\n",
    "df['HighCorr_Return_perception2'] = df['P1_Return_perception2'].where(df['P1_low_first'] == 0, df['P2_Return_perception2'])\n",
    "\n",
    "df['LowCorr_Return_perception3'] = df['P1_Return_perception3'].where(df['P1_low_first'] == 1, df['P2_Return_perception3'])\n",
    "df['HighCorr_Return_perception3'] = df['P1_Return_perception3'].where(df['P1_low_first'] == 0, df['P2_Return_perception3'])\n",
    "\n",
    "# Averages\n",
    "df['Avg_Return_perception1'] = (df['LowCorr_Return_perception1'] + df['HighCorr_Return_perception1']) / 2\n",
    "df['Avg_Return_perception2'] = (df['LowCorr_Return_perception2'] + df['HighCorr_Return_perception2']) / 2\n",
    "df['Avg_Return_perception3'] = (df['LowCorr_Return_perception3'] + df['HighCorr_Return_perception3']) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indirect-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjective risk perception\n",
    "df['LowCorr_Risk_perception1'] = df['P1_Risk_perception1'].where(df['P1_low_first'] == 1, df['P2_Risk_perception1'])\n",
    "df['HighCorr_Risk_perception1'] = df['P1_Risk_perception1'].where(df['P1_low_first'] == 0, df['P2_Risk_perception1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "minimal-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate each row to create First_Round/Second_Round rows\n",
    "df_expanded = pd.concat([df.assign(dup=0), df.assign(dup=1)], ignore_index=True)\n",
    "\n",
    "# Mark round\n",
    "df_expanded['First_Round'] = (df_expanded['dup'] == 0).astype(int)\n",
    "\n",
    "# Asset order indicators\n",
    "df_expanded['High_Corr'] = df_expanded.apply(\n",
    "    lambda row: 1 if (row['First_Round'] == 1 and row['P1_low_first'] == 0) or\n",
    "                     (row['First_Round'] == 0 and row['P1_low_first'] == 1) else 0,\n",
    "    axis=1\n",
    ")\n",
    "df_expanded['Low_Corr'] = 1 - df_expanded['High_Corr']\n",
    "\n",
    "# Share invested in low-return asset (converted to %)\n",
    "df_expanded['Share_lowreturn'] = df_expanded.apply(\n",
    "    lambda row: row['P1_investmentAsset_lowreturn'] if row['First_Round'] == 1 else row['P2_investmentAsset_lowreturn'],\n",
    "    axis=1\n",
    ")\n",
    "df_expanded['Share_lowreturn'] = df_expanded['Share_lowreturn'] / 10000 * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cheap-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate allocation shares to percentages\n",
    "df['LowCorr_investmentAsset_lowret'] = df['LowCorr_investmentAsset_lowret'] / 10000 * 100\n",
    "df['HighCorr_investmentAsset_lowret'] = df['HighCorr_investmentAsset_lowret'] / 10000 * 100\n",
    "\n",
    "# Correlation neglect and consideration\n",
    "df['correlation_neglect'] = df['HighCorr_investmentAsset_lowret'] - df['LowCorr_investmentAsset_lowret']\n",
    "df['consider_corr'] = df['LowCorr_investmentAsset_lowret'] - df['HighCorr_investmentAsset_lowret']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "voluntary-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of labels\n",
    "var_labels = {\n",
    "    'Own_asset': \"Owns Equity\",\n",
    "    'Intere_inves': \"Interested Financial Markets\",\n",
    "    'Trade_frequently': \"Frequent Trader\",\n",
    "    'High_wealth': \"High Wealth\",\n",
    "    'High_CurrentSustainShare': \"High SRI Share\",\n",
    "    'Invest_in_SRI': \"Invest in SRI\",\n",
    "    'Risk_taking': \"Risk Preferences\",\n",
    "    'Share_willing': \"Willingness to Share\",\n",
    "    'Donate_charity': \"Hypothetical Donation\",\n",
    "    'Social_Preferences': \"Social Preferences\",\n",
    "    'Donation': \"Incentivized Donation\",\n",
    "    'correct_Environ_Literacy': \"Environmental Literacy\",\n",
    "    'Sustainability_Value': \"Incentivized Sustainable Investment\",\n",
    "    'numcorrect_financialliteracyall': \"Financial Literacy\",\n",
    "    'correct_comprehension': \"Correlation Comprehension\",\n",
    "    'Stat_know': \"Statistics Knowledge\",\n",
    "    'Number_think': \"Doesn't Think about Numbers\",\n",
    "    'Number_inter': \"Consider Numbers in Decisions\",\n",
    "    'P1_low_first': \"Correlation Order\",\n",
    "    'P1_high_return': \"Asset Order\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "golden-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"transformed_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
